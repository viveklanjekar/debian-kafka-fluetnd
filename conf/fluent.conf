
# AUTOMATICALLY GENERATED
# DO NOT EDIT THIS FILE DIRECTLY, USE /templates/conf/fluent.conf.erb

@include "#{ENV['FLUENTD_SYSTEMD_CONF'] || 'systemd'}.conf"
@include "#{ENV['FLUENTD_PROMETHEUS_CONF'] || 'prometheus'}.conf"
@include kubernetes.conf
@include conf.d/*.conf


<match **>
  @type kafka2
  @log_level "#{ENV['FLUENT_KAFKA_OUTPUT_LOG_LEVEL'] || 'fatal'}"
  @id out_kafka
  brokers "#{ENV['FLUENT_KAFKA_BROKERS']}"
  default_topic "#{ENV['FLUENT_KAFKA_DEFAULT_TOPIC'] || nil}"
  <format>
    @type "#{ENV['FLUENT_KAFKA_OUTPUT_DATA_TYPE'] || 'json'}"
  </format>
  <inject>
    tag_key tag
    time_key time
  </inject>
  <buffer topic>
    @type "#{ENV['FLUENT_KAFKA_OUTPUT_BUFFER_TYPE'] || 'file'}"
    retry_timeout "#{ENV['FLUENT_KAFKA_OUTPUT_BUFFER_RETRY_TIMEOUT'] || '1h'}"
    overflow_action "#{ENV['FLUENT_KAFKA_OUTPUT_BUFFER_OVERFLOW_ACTION'] || 'drop_oldest_chunk'}"
    flush_at_shutdown "#{ENV['FLUENT_KAFKA_OUTPUT_BUFFER_FLUSH_SHUTDOWN'] || 'false'}"
    flush_interval "#{ENV['FLUENT_KAFKA_OUTPUT_BUFFER_FLUSH_INTERVAL'] || '10s'}"
    chunk_limit_size "#{ENV['FLUENT_KAFKA_OUTPUT_BUFFER_CHUNK_SIZE'] || '8MB'}"
    queued_chunks_limit_size "#{ENV['FLUENT_KAFKA_OUTPUT_BUFFER_CHUNK__QUEUE_LIMIT'] || '500'}"
    total_limit_size "#{ENV['FLUENT_KAFKA_OUTPUT_BUFFER_TOTAL_LIMIT_SIZE'] || '2GB'}"
    path "#{ENV['FLUENT_KAFKA_OUTPUT_BUFFER_PATH'] || '/tmp/kafka-data.*.buffer'}"
  </buffer>
  max_send_retries "#{ENV['FLUENT_KAFKA_MAX_SEND_RETRIES'] || 1}"
  required_acks "#{ENV['FLUENT_KAFKA_REQUIRED_ACKS'] || -1}"
  compression_codec "#{ENV['FLUENT_KAFKA_COMPRESSION_CODEC'] || gzip}"
</match>
